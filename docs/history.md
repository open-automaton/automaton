Where did this come from?
-------------------------

This technique was created to combat information traders who sell your personal information. Dating all the way back to Yahoo's "kevdb" database, which was then augmented at Microsoft and Infospace, then fragmented into a wide array of little companies(Intellius, whitepages, etc... all powered by this database), the Merlin database and later (after the intelligence community wanted into this pool) Axciom. They rebuild their databases (thus nullifying your "removal" request) monthly or quarterly, so I needed a robust monitoring and interaction solution that was simple enough to maintain that I could train people in XML, regex and xpath then give them the tools to maintain scripts. This is why the examples refer to real-world cases monitoring or removing your presence in these lists.

Automaton(and later, strip-mine) started out as a formalization of these techniques I have been using to scrape the web for almost a decade now, but as I ported it to JS from PHP & Java, a CTO surprised me with some requirements: GPLv3 and a non generalizable server configuration. I realized there were only 4 sources to scrape once per quarter (In other words: they had no need for scrapers in the first place, much less those designed to be a continual data conduit and minimize breakage). They also did not realize the patent implications of using GPLv3 and promptly buried it. Afterwards, it became frozen in time.

I've been continually taking questions about it recently, so it was worthwhile to update some even older code to replace the strip-mine ecosystem.
